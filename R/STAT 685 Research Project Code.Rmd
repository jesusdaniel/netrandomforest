---
title: "STAT 685 Research Project"
author: "Tiffany Chang"
date: "2023-09-05"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries and custom functions

```{r}
## Required libraries
library(Matrix)
library(MASS)
library(igraphdata)
library(igraph)
library(sand)
library(devtools)
library(dplyr)
# library(graphclass)
library(caret)
library(naniar)
library(tidyr)
library(ggplot2)
library(reshape2)
library(rARPACK)
library(rpart)
library(varhandle)
library(randomForest)
library(rerf)
library(e1071)

## Custom functions
## Link for syntax on creating functions in different R scripts: https://github.com/jesusdaniel/dcmase
source("Plots.R")
source("extra_functions.R")
source("matrix_to_vec.R")
source("mase.R")
source("ase.R")
source("clustering_vertices.R")
source("cluster_testing.R")
# source("rpart_gini.R")
source("rf_training.R")
source("rf_testing.R")
source("Mode.R")
source("rf_training2.R")
source("rf_testing2.R")
source("generate_data.R")
# source("data_summary.R")
# install_github("jesusdaniel/graphclass") # Only needed to do this step once
```

## Reading in Cobre Data

```{r}
load("COBRE.data.rda")
subjects_df=COBRE.data
```

## Creating a dataframe of mouse data (124 x 34453)

```{r}
subjects_df=data.frame(subject_id=subjects_df$subject.label,health_status=subjects_df$Y.cobre,brain_edge=subjects_df$X.cobre[c(1:124),])

head(subjects_df,n=10) # Showing the first 10 subjects

subjects_df2=subjects_df[1:10,1:6]
```


## Generate plots from neural network data

```{r}
## Visualizing brain data from the 1st and 2nd subjects
adj_matrix1=get_matrix(as.numeric(subjects_df[1,c(2:34455)])) # First subject
adj_matrix2=get_matrix(as.numeric(subjects_df[2,c(2:34455)])) # Second subject

## Build the graph object
network1=graph_from_adjacency_matrix(adj_matrix1 > 0.5)
network2=graph_from_adjacency_matrix(adj_matrix2 > 0.5)
 
## Plot it
igraph_options(vertex.size=10)
par(mfrow=c(1,2))
plot(network1, vertex.label.cex=.5)
title(main="Brain Network of Subject 1")
igraph_options(vertex.size=10)
plot(network2, vertex.label.cex=.5)
title(main="Brain Network of Subject 2")
```
## Exploratory Data Analysis

```{r}
## Calculating the correlation between each possible pair of features
# round(cor(subjects_df[,c(2:34455)]),digits=3) # Takes too long
# pairwise_cor=corSelect(subjects_df, sp.cols = 2, var.cols=3:34455, coeff = TRUE, cor.thresh = 0.5,
# select = "cor", family = "auto",
# use = "pairwise.complete.obs", method = "pearson")

# save(pairwise_cor,file="Pairwise Correlation Results.Rdata")
# load("Pairwise Correlation Results.Rdata")

# pairwise_cor$high.correlations
# pairwise_cor$excluded.vars ## the variable was excluded based on the smallest correlation between the response
# pred_selected=pairwise_cor$selected.var.cols

## Reduce the dataset using the selected variables
# subjects_df=subjects_df[,pred_selected]

## Comment: Cannot perform pairwise correlations on raw data since it takes far too long (over 24 hours). Instead I will perform pairwise correlation
```


## Implementing 10-fold Cross Validation (manual random selection)

```{r}
## Implementing cross validation
# Create a list of testing and training data
testing_data_list=list()
training_data_list=list()
remaining_data=c(1:124)
sampled_vec=c()

## Note, if we want 10 folds, we will need to split the data as evenly as possible. One such way is 12 subjects for each of the first 6 folds and 13 subjects for each of the remaining 4 folds.
# Folds: Specifies the indicies that correspond to testing and training data for each fold.
# Fold 1:
set.seed(45)
testing_data_list[[1]]=sample(c(1:124),12)
sampled_vec=c(testing_data_list[[1]])
training_data_list[[1]]=(1:124)[-testing_data_list[[1]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 2:
set.seed(45)
testing_data_list[[2]]=sample(remaining_data,12)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]])
training_data_list[[2]]=(1:124)[-testing_data_list[[2]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 3:
set.seed(45)
testing_data_list[[3]]=sample(remaining_data,12)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]])
training_data_list[[3]]=(1:124)[-testing_data_list[[3]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 4:
set.seed(45)
testing_data_list[[4]]=sample(remaining_data,12)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]],testing_data_list[[4]])
training_data_list[[4]]=(1:124)[-testing_data_list[[4]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 5:
set.seed(45)
testing_data_list[[5]]=sample(remaining_data,12)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]],testing_data_list[[4]],testing_data_list[[5]])
training_data_list[[5]]=(1:124)[-testing_data_list[[5]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 6:
set.seed(45)
testing_data_list[[6]]=sample(remaining_data,12)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]],testing_data_list[[4]],testing_data_list[[5]],testing_data_list[[6]])
training_data_list[[6]]=(1:124)[-testing_data_list[[6]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 7:
set.seed(45)
testing_data_list[[7]]=sample(remaining_data,13)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]],testing_data_list[[4]],testing_data_list[[5]],testing_data_list[[6]],testing_data_list[[7]])
training_data_list[[7]]=(1:124)[-testing_data_list[[7]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 8:
set.seed(45)
testing_data_list[[8]]=sample(remaining_data,13)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]],testing_data_list[[4]],testing_data_list[[5]],testing_data_list[[6]],testing_data_list[[7]],testing_data_list[[8]])
training_data_list[[8]]=(1:124)[-testing_data_list[[8]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 9:
set.seed(45)
testing_data_list[[9]]=sample(remaining_data,13)
sampled_vec=c(testing_data_list[[1]],testing_data_list[[2]],testing_data_list
[[3]],testing_data_list[[4]],testing_data_list[[5]],testing_data_list[[6]],testing_data_list[[7]],testing_data_list[[8]],testing_data_list[[9]])
training_data_list[[9]]=(1:124)[-testing_data_list[[9]]]
remaining_data=(1:124)[-sampled_vec]
# Fold 10:
set.seed(45)
testing_data_list[[10]]=remaining_data
training_data_list[[10]]=(1:124)[-testing_data_list[[10]]]
```


## Training standard random forest classification on each fold (cluster and perform pairwise correlation *before* fitting the standard RF model and without any subsampling)

```{r}
## Create a matrix to store mean and sd classification accuracy for each random forest method to later be converted to a dataframe object
### col 1 = "RF_Method" ("RF", "NRF1", or "NRF2"), col 2 = "Number_of_Clusters" (10-15, inclusive), col 3 = "Mean_Accuracy", col 4 = "SD_Accuracy"
### rows 1-6 = mean and sd accuracy for every cluster number for "RF" method, rows 7-12 = mean and sd accuracy for every cluster number for "NRF1" method, rows 13-18 = mean and sd accuracy for every cluster number for "NRF2" method
rf_methods_all=matrix(ncol=4,nrow=6*3)

## Implementing RF
rf_method="RF"

## Create a list to store the clustered dataframes generated from 10,...,15 clusters
clusterdf_list=list()

## Create a list to store the pairwise correlations of all clustered dataframes
pairwisecor_list=list()

for (n in 1:6){ # n is supposed to specify the number of clusters; however it is also used to iterate the rows to be populated into the dataframe, so will have to add 9 to all n indicies in order to cluster the data using n = 10 to n = 15
  
  ## Cluster the data using (n + 9) number of clusters
  num_clusters=n+9

  ## Create the adjacency list to be fed into the clustering function
  adj_list=list()
  for(i in 1:124){
    A=get_matrix(as.numeric(subjects_df[i,c(3:ncol(subjects_df))]))
    adj_list[[i]]=A
  }
  
  ## Clustering data into num_clusters
  set.seed(45)
  clusters <- clustering_vertices(adj_list,num_clusters)
  clustering_result=clusters$X_clusters
  clustering_memberships=clusters$cluster_memberships
  Xnew1=ifelse(is.nan(clustering_result),0,clustering_result)
  cluster_df=data.frame(health_status=subjects_df[,2],Xnew1) # The response is the patients' mental health status (healthy = -1 and schizophrenic = 1)
  clusterdf_list[[n]]=cluster_df
  
  ## Perform pairwise correlation analysis
  pairwise_cor=corSelect(cluster_df, sp.cols = 1, var.cols=2:ncol(cluster_df), coeff = TRUE, 
  cor.thresh = 0.5,
  select = "cor", family = "auto",
  use = "pairwise.complete.obs", method = "pearson")
  pairwisecor_list[[n]]=pairwise_cor
  pred_selected=pairwise_cor$selected.var.cols # Variables that were not excluded

  ## Reduce the dataset using the selected variables
  clusterred_df=cluster_df[,c(1,pred_selected)]
  
  ## Create a vector to store classification results per fold
  classification_results=rep(NA, 10)
    
  for(i in 1:10) {
    training_data=clusterred_df[training_data_list[[i]],]
    testing_data=clusterred_df[testing_data_list[[i]],]
    
    ## Train classifier on training_data_list[[i]]
    set.seed(45)
    rf_mod1 <- randomForest(x=training_data[,c(2:ncol(training_data))],
                       y=factor(training_data[,1]),
                       ntree=10,
                       importance=TRUE)
    
    ## Test classifier on testing_data_list[[i]]
    test=predict(rf_mod1,newdata=testing_data[,c(1:ncol(testing_data))],type='response')
    
    ## Store result on classification_results[i]
    classification_results[i]=1-mean(test != testing_data$health_status)
  }
  
  avg_acc_RF=mean(classification_results) # Average cross-validated classification accuracy across all 10 folds
  sd_acc_RF=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds
  
  ## Populate the rf_methods_all dataframe row by row
  rf_methods_all[n,]=c(rf_method,num_clusters,avg_acc_RF,sd_acc_RF)
}

## Viewing which variables were excluded for the clustered dataset generated from 10 clusters
pairwisecor_list[[1]]$excluded.vars ## the variable was excluded based on the smallest correlation between the response

pairwisecor_list[[1]]$high.correlations

## Plot the pairwise correlations for each cluster number (6 total)
par(mfrow=c(2,3))

cor1=cor(clusterdf_list[[1]][,2:ncol(clusterdf_list[[1]])])
corrplot(cor1, type = "upper", diag=FALSE, title="10 Clusters", order = "hclust", tl.pos='n', mar=c(0,0,1,0)) ## Pairwise correlations for num_clusters = 10

cor2=cor(clusterdf_list[[2]][,2:ncol(clusterdf_list[[2]])])
corrplot(cor2, type = "upper", diag=FALSE, title="11 Clusters", order = "hclust", tl.pos='n', mar=c(0,0,1,0))

cor3=cor(clusterdf_list[[3]][,2:ncol(clusterdf_list[[3]])])
corrplot(cor3, type = "upper", diag=FALSE, title="12 Clusters", order = "hclust", tl.pos='n', mar=c(0,0,1,0))

cor4=cor(clusterdf_list[[4]][,2:ncol(clusterdf_list[[4]])])
corrplot(cor4, type = "upper", diag=FALSE, title="13 Clusters", order = "hclust", tl.pos='n', mar=c(0,0,1,0))

cor5=cor(clusterdf_list[[5]][,2:ncol(clusterdf_list[[5]])])
corrplot(cor5, type = "upper", diag=FALSE, title="14 Clusters", order = "hclust", tl.pos='n', mar=c(0,0,1,0))

cor6=cor(clusterdf_list[[6]][,2:ncol(clusterdf_list[[6]])])
corrplot(cor6, type = "upper", diag=FALSE, title="15 Clusters", order = "hclust", tl.pos='n', mar=c(0,0,1,0))
```


## Training custom random forest function with clustering outside of the trees (different clustered edge predictors per fold)

```{r}
## Implementing NRF1
rf_method="NRF1"

clusterdf_list=list()
pairwisecor_list=list()

for (n in 7:12){ # n is supposed to specify the number of clusters; however it is also used to iterate the rows to be populated into the dataframe, so will have to add 3 to all n indicies in order to cluster the data using n = 10 to n = 15
  
  # Cluster the data using (n + 3) number of clusters
  num_clusters=n+3
  
  for(i in 1:10) {
    training_data=subjects_df[training_data_list[[i]],]
    testing_data=subjects_df[testing_data_list[[i]],]
    
    # Train classifier on training_data_list[[i]]
    rf_mod2=rf_training(B=10,n=lengths(training_data_list)[i],X_training=training_data,num_clusters=num_clusters,prop_vert=.8) 
    
    clusterdf_list[[(n-6)*i]]=rf_mod2$clusterdf
    pairwisecor_list[[(n-6)*i]]=rf_mod2$pairwisecor
    
    # Test classifier on testing_data_list[[i]]
    test=rf_testing_pred(X_testing=testing_data,rf_training=rf_mod2,num_clusters=num_clusters)
    
    # Store result on classification_results[i]
    classification_results[i]=1-mean(test != testing_data$health_status)
  }

  avg_acc_NRF1=mean(classification_results) # Average cross-validated classification accuracy across all 10 folds
  sd_acc_NRF1=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds
  
  ## Populate the rf_methods_all dataframe row by row
  rf_methods_all[n,]=c(rf_method,num_clusters,avg_acc_NRF1,sd_acc_NRF1)
}

## Viewing which variables were excluded for the clustered dataset generated from 10 clusters and on fold #1
pairwisecor_list[[1]]$excluded.vars ## the variable was excluded based on the smallest correlation between the response

## Plot the pairwise correlations for num_clusters=10 and fold #1
cor1_NRF1=cor(clusterdf_list[[1]][,2:ncol(clusterdf_list[[1]])])
corrplot(cor1_NRF1, type = "upper", diag=FALSE, title="10 Clusters, Fold #1", order = "hclust", tl.pos='n', mar=c(0,0,1,0))
```

## Training custom random forest function with clustering within each tree (different clustered edge predictors per fold and per decision tree)

```{r}
## Implementing NRF2
rf_method="NRF2"

clusterdf_list=list()
pairwisecor_list=list()

for (n in 13:18){ # n is supposed to specify the number of clusters; however it is also used to iterate the rows to be populated into the dataframe, so will have to subtract 3 to all n indicies in order to cluster the data using n = 10 to n = 15
  
  # Cluster the data using (n - 3) number of clusters
  num_clusters=n-3
  for(i in 1:10) {
    training_data=subjects_df[training_data_list[[i]],]
    testing_data=subjects_df[testing_data_list[[i]],]
    
    # Train classifier on training_data_list[[i]]
    rf_mod3=rf_training2(B=10,n=lengths(training_data_list)[i],X_training=training_data,num_clusters=num_clusters,prop_vert=.8) 
    
    clusterdf_list[[(n-12)*i]]=rf_mod3$clusterdf # Store the list of clustered datasets for each decision tree at the ith fold and num_clusters
    pairwisecor_list[[(n-12)*i]]=rf_mod3$pairwisecor # Store the list of pairwise correlations for each decision tree at the ith fold and num_clusters
    
    # Test classifier on testing_data_list[[i]]
    test=rf_testing_pred2(X_testing=testing_data,rf_training=rf_mod3,num_clusters=num_clusters)
    
    # Store result on classification_results[i]
    classification_results[i]=1-mean(test != testing_data$health_status)
  }

  avg_acc_NRF2=mean(classification_results) # Average cross-validated classification accuracy across all 10 folds
  sd_acc_NRF2=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds
  
  ## Populate the rf_methods_all dataframe row by row
  rf_methods_all[n,]=c(rf_method,num_clusters,avg_acc_NRF2,sd_acc_NRF2)
}

## Convert to a dataframe object, add column names, and display the results
rf_methods_all=data.frame(rf_methods_all)
colnames(rf_methods_all)=c("RF_Method","Number_of_Clusters","Mean_Accuracy","SD_Accuracy")
rf_methods_all

save(rf_methods_all,file="RF Models Results.Rdata")
load("RF Models Results.Rdata")

## Viewing which variables were excluded for the clustered dataset generated from 10 clusters, fold #1, and decision tree #1
pairwisecor_list[[1]][[1]]$excluded.vars ## the variable was excluded based on the smallest correlation between the response

## Plot the pairwise correlations for num_clusters=10, fold #1, and decision tree #1
cor1_NRF2=cor(clusterdf_list[[1]][[1]][,2:ncol(clusterdf_list[[1]][[1]])])
corrplot(cor1_NRF2, type = "upper", diag=FALSE, title="10 Clusters, Fold #1, Decision Tree #1", order = "hclust", tl.pos='n', mar=c(0,0,1,0))

## Make side-by-side plots for comparisons of the correlation plots between the 3 methods
par(mfrow=c(1,3))

corrplot(cor1, type = "upper", diag=FALSE, title="RF", order = "hclust", tl.pos='n', mar=c(0,0,1,0)) ## Pairwise correlations for num_clusters = 10

corrplot(cor1_NRF1, type = "upper", diag=FALSE, title="NRF1", order = "hclust", tl.pos='n', mar=c(0,0,1,0))

corrplot(cor1_NRF2, type = "upper", diag=FALSE, title="NRF2", order = "hclust", tl.pos='n', mar=c(0,0,1,0))
```


## Graphing the results 

```{r}
(rf_line_graph=ggplot(rf_methods_all,aes(x=Number_of_Clusters,y=as.numeric(Mean_Accuracy),colour=RF_Method,group=RF_Method)) +
  geom_line(size=1) +
  ylim(.51,.67) + 
  labs(x = "Number of Clusters", 
       y = "Mean Classification Accuracy", 
       color = "RF Method\n") + 
  ggtitle("Random Forest Models vs. Number of Clusters\n") +
  theme(plot.title = element_text(hjust = 0.5)))

ggsave("rf_line_graph.png", plot = rf_line_graph)
```

## Variable Importance Calculations

```{r}
## Since NRF2 yielded the highest classification accuracy at 12 clusters, we will compute the VI from that model
## First, obtain the model
rf_list=list() # Store the rf objects across all 10 folds

for(i in 1:10) {
  training_data=subjects_df[training_data_list[[i]],]
  testing_data=subjects_df[testing_data_list[[i]],]
  
  # Train classifier on training_data_list[[i]]
  rf_mod_final=rf_training2(B=10,n=lengths(training_data_list)[i],X_training=training_data,num_clusters=12,prop_vert=.8)
  
  rf_list[[i]]=rf_mod_final
  
  # Test classifier on testing_data_list[[i]]
  test=rf_testing_pred2(X_testing=testing_data,rf_training=rf_mod_final,num_clusters=12)
  
  # Store result on classification_results[i]
  classification_results[i]=1-mean(test != testing_data$health_status)
}

classification_results # See which fold had the highest predictive accuracy to determine which model to compute the VI on
  
## The rf model that yielded the highest predictive accuracy was rf model at fold #1
rf_mod_final=rf_list[[1]]

vec_VI_list=list() # Define a list that stores the variable importances of each tree in the final random forest model
for(i in 1:length(rf_mod_final$rf)){
  vec_VI=rf_mod_final$rf[[i]]$variable.importance
  vec_VI_list[[i]]=vec_VI[order(factor(names(vec_VI)))]
}

## Create a dataframe that scores all the variable importances from each tree
VI_df=data.frame(matrix(0,nrow=10,ncol=78))

## Fill the dataframe with nonzero values of the corresponding variable importances
for(i in 1:length(vec_VI_list)){
  for (j in 1:length(vec_VI_list[[i]])){
    for (k in 1:ncol(VI_df)){
      if(names(vec_VI_list[[i]][j])==colnames(VI_df)[k]){
        VI_df[i,k]=vec_VI_list[[i]][j]
      }
    }
  }
}

(mean_VI=colMeans(VI_df))

## Find the verticies corresponding to the most predictive clustered predictors
NRF2_importance=mean_VI
VI_mat <- matrix(0, 12, 12)
VI_mat[upper.tri(VI_mat, diag = TRUE)] = NRF2_importance
VI_mat # View the matrix
plot_adjmatrix(VI_mat)  
(which(VI_mat == max(VI_mat),arr.ind=T))

mean_VI_vec=c()
## Find which decision tree had the highest mean variable importance
for(i in 1:length(rf_mod_final$rf)){
  mean_VI_vec=c(mean_VI_vec, mean(rf_mod_final$rf[[i]]$variable.importance))
}
which(mean_VI_vec==max(mean_VI_vec)) # The 5th decision tree in the final random forest model has the highest mean variable importance, so we will obtain the vertices belonging to memberships 2 and 3 (most significant clustered edge predictors)


(rf_included=unique(rf_included)) # vertices belonging to the top clustered brain edge predictors across all clustered datasets of each tree in the random forest

## Obtain the brain regions corresponding to these vertices
data("power.parcellation")
brain_regions=power.parcellation[-75, ]
unique(brain_regions[rf_included,3]) # Unique brain regions that correspond to the top edge predictors
```
## Create stratified folds

```{r}
## Implementing stratified cross validation
# Create a list of testing and training data
set.seed(45)
testing_data_list=createFolds(subjects_df$health_status, k = 10, list = TRUE, returnTrain = FALSE)

## Populate the training data folds
training_data_list=list(Fold1=c(),Fold2=c(),Fold3=c(),Fold4=c(),Fold5=c(),Fold6=c(),Fold7=c(),Fold8=c(),Fold9=c(),Fold10=c())
for(k in 1:10){
  training_data_list[[k]]=(1:124)[-testing_data_list[[k]]]
}
```


## Fit the SVM, standard, and SPORF RF methods on the raw data

```{r}
classification_results=c()

## Standard RF
for(i in 1:10) {  
  training_data=subjects_df[training_data_list[[i]],]
  testing_data=subjects_df[testing_data_list[[i]],]
      
  ## Train classifier on training_data_list[[i]]
  set.seed(45)
  rf <- randomForest(x=training_data[,c(3:ncol(training_data))],
                     y=factor(training_data[,2]),
                     ntree=10,
                     importance=TRUE)
  ## Test classifier on testing_data_list[[i]]
  test=predict(rf,newdata=testing_data[,c(2:ncol(testing_data))],type='response')
  
  # Store result on classification_results[i]
  classification_results[i]=1-mean(test != testing_data$health_status)
}
    
## Store results
avg_acc_RF=mean(classification_results) # Store classification accuracy of the standard RF method
sd_acc_RF=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds


## SPORF
for(i in 1:10) {  
  training_data=subjects_df[training_data_list[[i]],]
  testing_data=subjects_df[testing_data_list[[i]],]
      
  ## Train classifier on training_data_list[[i]]
  sporf <- RerF(X=training_data[,c(3:ncol(training_data))], 
                  Y=factor(training_data[,2]), 
                  trees=10,
                  num.cores = 1L,
                  seed = 45)
  test <- Predict(testing_data[,c(2:ncol(testing_data))], sporf, num.cores = 1L, Xtrain = 
                  training_data[,c(3:ncol(training_data))])
  
  # Store result on classification_results[i]
  classification_results[i]=1-mean(test != testing_data$health_status)
}
    
## Store results
avg_acc_SPORF=mean(classification_results) # Store classification accuracy of the SPORF method
sd_acc_SPORF=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 5 folds

## SVM
for(i in 1:10) {
  training_data=subjects_df[training_data_list[[i]],]
  testing_data=subjects_df[testing_data_list[[i]],]
  
  # Train classifier on training_data_list[[i]]
  set.seed(45)
  svm_mod=train(
    as.factor(health_status)~.,
    data=training_data[,c(2:ncol(training_data))],
    method="svmLinear2"
  )
  
  # Test classifier on testing_data_list[[i]]
  test=predict(svm_mod,newdata=testing_data[,c(2:ncol(testing_data))],type='raw')
  
  # Store result on classification_results[i]
  classification_results[i]=1-mean(test != testing_data$health_status)
}
  
# Cross-validated classification accuracy:
avg_acc_SVM=mean(classification_results)
sd_acc_SVM=sd(classification_results)
```

## Implemented nested cross validation for NRF1 and NRF2

```{r}
classification_results=c()

## NRF1
for(j in 1:10){ # Fold j (validation)
  #browser()
  non_j_ind=c(1:10)[-j]
  k_df=data.frame(matrix(ncol = 30, nrow = 4)) # Empty i x k dataframe to store all predictive accuracies
  k_vec=c() # Vector to store all average predictive accuracies for all k
  k_star=0 # Define hyperparameter for number of clusters
  for(i in non_j_ind){ # Fold i (testing)
    valid_results=rep(-Inf,30)
    training_data1=subjects_df[-c(testing_data_list[[i]],testing_data_list[[j]]),] # Training data (excluding both the jth and ith folds)
    valid_data=subjects_df[testing_data_list[[i]],] # ith fold for validation set
    for(k in 2:15){ # Number of clusters
      nrf1_valid=rf_training(B=10,n=nrow(training_data1),X_training=training_data1,num_clusters=k,prop_vert=.8)
      valid=rf_testing_pred(X_testing=valid_data,rf_training=nrf1_valid,num_clusters=k)
      valid_results[k]=1-mean(valid != valid_data$health_status) # For each fold, store the predictive accuracy for all values of k
    }
    k_df[i,]=valid_results
  }
  ## Take columnwise means across all i folds
  k_vec=colMeans(k_df,na.rm=TRUE)
  k_star=which.max(k_vec) # Obtain the index (i.e. value of k) that yielded the maximum predictive accuracy for the ith fold
  training_data2=subjects_df[training_data_list[[j]],] # Training data (excluding only the jth fold)
  testing_data=subjects_df[testing_data_list[[j]],] # jth fold for testing set
  nrf1_test=rf_training(B=10,n=nrow(training_data2),X_training=training_data2,num_clusters=k_star,prop_vert=.8)
  test=rf_testing_pred(X_testing=testing_data,rf_training=nrf1_test,num_clusters=k_star)
  classification_results[j]=1-mean(test != testing_data$health_status) # For each fold, store the predictive accuracy for all values of k
}

## Store results
avg_acc_NRF1=mean(classification_results) # Store classification accuracy of the NRF1 method
sd_acc_NRF1=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds


## NRF2
for(j in 1:10){ # Fold j (validation)
  #browser()
  non_j_ind=c(1:10)[-j]
  k_df=data.frame(matrix(ncol = 30, nrow = 4)) # Empty i x k dataframe to store all predictive accuracies
  k_vec=c() # Vector to store all average predictive accuracies for all k
  k_star=0 # Define hyperparameter for number of clusters
  for(i in non_j_ind){ # Fold i (testing)
    valid_results=rep(-Inf,30)
    training_data1=subjects_df[-c(testing_data_list[[i]],testing_data_list[[j]]),] # Training data (excluding both the jth and ith folds)
    valid_data=subjects_df[testing_data_list[[i]],] # ith fold for validation set
    for(k in 2:15){ # Number of clusters
      valid=rf_testing_pred(X_testing=valid_data,rf_training=nrf2_valid,num_clusters=k)
      valid_results[k]=1-mean(valid != valid_data$health_status) # For each fold, store the predictive accuracy for all values of k
    }
    k_df[i,]=valid_results
  }
  ## Take columnwise means across all i folds
  k_vec=colMeans(k_df,na.rm=TRUE)
  k_star=which.max(k_vec) # Obtain the index (i.e. value of k) that yielded the maximum predictive accuracy for the ith fold
  training_data2=subjects_df[training_data_list[[j]],] # Training data (excluding only the jth fold)
  testing_data=subjects_df[testing_data_list[[j]],] # jth fold for testing set
  nrf2_test=rf_training(B=10,n=nrow(training_data2),X_training=training_data2,num_clusters=k_star,prop_vert=.8)
  test=rf_testing_pred(X_testing=testing_data,rf_training=nrf2_test,num_clusters=k_star)
  classification_results[j]=1-mean(test != testing_data$health_status) # For each fold, store the predictive accuracy for all values of k
}

## Store results
avg_acc_NRF2=mean(classification_results) # Store classification accuracy of the NRF2 method
sd_acc_NRF2=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds
```
## Create a dataframe of results

```{r}
results_df=data.frame(RF=c(avg_acc_RF),SPORF=c(avg_acc_SPORF),SVM=c(avg_acc_SVM),NRF1=c(avg_acc_NRF1),NRF2=c(avg_acc_NRF2))
row.names(results_df) <- c("COBRE") # Rows correspond to the dataset used
## This table will be updated as these quantities are computed on new datasets
```

## Graph the results for NRF1 and NRF2 for k=2,...,15 clusters

```{r}
## Create a matrix to store mean and sd classification accuracy for each random forest method to later be converted to a dataframe object
### col 1 = "RF_Method" ("NRF1" or "NRF2"), col 2 = "Number_of_Clusters" (2-15, inclusive), col 3 = "Mean_Accuracy", col 4 = "SD_Accuracy"
### rows 1-14 = mean and sd accuracy for every cluster number for "RF" method, rows 15-28 = mean and sd accuracy for every cluster number for "NRF1" method, rows 13-18 = mean and sd accuracy for every cluster number for "NRF2" method
rf_methods_all=matrix(ncol=4,nrow=14*2)

## Implementing RF
rf_method="NRF1"

## Create a list to store the clustered dataframes generated from 10,...,15 clusters
clusterdf_list=list()

## Create a list to store the pairwise correlations of all clustered dataframes
pairwisecor_list=list()

for (n in 1:14){ # n is supposed to specify the number of clusters; however it is also used to iterate the rows to be populated into the dataframe, so will have to add 9 to all n indicies in order to cluster the data using n = 2 to n = 15
  
  classification_results=c()
  
  ## Cluster the data using (n + 1) number of clusters
  num_clusters=n+1

  for(i in 1:10) {
    training_data=subjects_df[training_data_list[[i]],]
    testing_data=subjects_df[testing_data_list[[i]],]
    
    # Train classifier on training_data_list[[i]]
    rf_mod2=rf_training(B=10,n=lengths(training_data_list)[i],X_training=training_data,num_clusters=num_clusters,prop_vert=.8) 
    
    # Test classifier on testing_data_list[[i]]
    test=rf_testing_pred(X_testing=testing_data,rf_training=rf_mod2,num_clusters=num_clusters)
    
    # Store result on classification_results[i]
    classification_results[i]=1-mean(test != testing_data$health_status)
  }

  avg_acc_NRF1=mean(classification_results) # Average cross-validated classification accuracy across all 10 folds
  sd_acc_NRF1=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds
  
  ## Populate the rf_methods_all dataframe row by row
  rf_methods_all[n,]=c(rf_method,num_clusters,avg_acc_NRF1,sd_acc_NRF1)
}

## Implementing NRF2
rf_method="NRF2"

for (n in 15:28){ # n is supposed to specify the number of clusters; however it is also used to iterate the rows to be populated into the dataframe, so will have to subtract 13 to all n indicies in order to cluster the data using n = 2 to n = 15
  
  classification_results=c()
  
  # Cluster the data using (n - 13) number of clusters
  num_clusters=n-13
  
  for(i in 1:10) {
    training_data=subjects_df[training_data_list[[i]],]
    testing_data=subjects_df[testing_data_list[[i]],]
    
    # Train classifier on training_data_list[[i]]
    rf_mod3=rf_training2(B=10,n=lengths(training_data_list)[i],X_training=training_data,num_clusters=num_clusters,prop_vert=.8) 
    
    # Test classifier on testing_data_list[[i]]
    test=rf_testing_pred2(X_testing=testing_data,rf_training=rf_mod3,num_clusters=num_clusters)
    
    # Store result on classification_results[i]
    classification_results[i]=1-mean(test != testing_data$health_status)
  }

  avg_acc_NRF2=mean(classification_results) # Average cross-validated classification accuracy across all 10 folds
  sd_acc_NRF2=sd(classification_results) # Standard deviation of cross-validated classification accuracy across all 10 folds
  
  ## Populate the rf_methods_all dataframe row by row
  rf_methods_all[n,]=c(rf_method,num_clusters,avg_acc_NRF2,sd_acc_NRF2)
}


## Convert to a dataframe object, add column names, and display the results
rf_methods_all=data.frame(rf_methods_all)
colnames(rf_methods_all)=c("RF_Method","Number_of_Clusters","Mean_Accuracy","SD_Accuracy")
rf_methods_all

save(rf_methods_all,file="RF Models Results.Rdata")
load("RF Models Results.Rdata")

## Plot the results
(rf_line_graph=ggplot(rf_methods_all,aes(x=Number_of_Clusters,y=as.numeric(Mean_Accuracy),colour=RF_Method,group=RF_Method)) +
  geom_line(size=1) +
  ylim(.51,.67) + 
  labs(x = "Number of Clusters", 
       y = "Mean Classification Accuracy", 
       color = "RF Method\n") + 
  ggtitle("Random Forest Models vs. Number of Clusters\n") +
  theme(plot.title = element_text(hjust = 0.5)))
```

