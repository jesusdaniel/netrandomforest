---
title: "Spectral Clustering Dimensional Reduction Within Random Forest Models"
author: Tiffany Chang
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Spectral Clustering Dimensional Reduction Within Random Forest Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

This vignette is designed to explain in-depth about the purpose and usage of the netRandomForest package. Overall, it is used to perform spectral clustering within a random forest model in order to reduce the dimensionality of a data set with a large number of predictors. In this document, it is built on R simulated data from the Stochastic Block Model (SBM). The net random forest models are meant to reduce the running time for classifying each observation to one of the two categories of the binary response of interest (more details in the next section), despite having potentially thousands of predictors in the SBM data set. Thus, this example demonstrates a classification problem that integrates dimensional reduction to achieve maximum efficiency while retaining a reasonable predictive performance.

We will use the following packages and source files:

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup,message=FALSE,warning=FALSE}
## Required libraries
library(netRandomForest)
library(randomForest)
library(rerf)
library(Matrix)
library(MASS)
library(lattice)
library(igraphdata)
library(igraph)
library(sand)
library(devtools)
library(dplyr)
library(caret)
library(naniar)
library(tidyr)
library(ggplot2)
library(reshape2)
library(rARPACK)
library(varhandle)
library(ROCR)

## Source functions
source("../../Plots.R")
source("../../extra_functions.R")
```

## Data

To reiterate, the data is generated in R through a simulation of 100 networks from the SBM and simultaneously pre-processed within the generate_data_SBM custom function. The processed data set contains a very large number of binary predictors (0 or 1) and a binary response (1 for Class 1 or -1 for Class 2). The number of predictors is based on the number of vertices/nodes (n) in each of the generated SBM undirected graphs. In this case, we have n = 100 for our simulations, so the total number of variables is (n*(n-1))/2 = (100 x 99)/2 = 4950. There are three main goals to the overall analyses performed in this vignette: implement spectral clustering within random forest models to enhance the efficiency and quality of predictions, given the high dimensionality of this data set, identify important "blocks" or communities within the SBM data (using Gini-based variable importance metrics), and how "close" the important variables are to those of the true simulation (a performance measure gauged by the ROC curve). [Note: I don't think I did a great job explaining the variable importance portion.]

It is important to note that for each individual SBM simulation, the block model was pre-specified to have five communities (a 5 x 5 "block" matrix) with the diagonal blocks having a probability of 0.5 (p1) and the off-diagonal blocks having a probability of 0.1 (p2) [Note: Not sure if I am explaining this right]. These probabilities represent the likelihood of two different nodes having a shared edge within a certain community (1 if a pair of node is connected, and 0 otherwise). We define another probability, q, that is designated for generating different "classes" of SBM simulations by effectively changing one of the off-diagonal blocks to have a probability of greater than p1 and less than or equal to p2. For demonstration purposes below, we set q to be 0 for Class 1 and 0.4 for Class 2. The data was originally a list of 100 SBM graphs that were converted to adjacency matrices, 50 from Class 1 (m1) and the other 50 from Class 2 (m2). In order to pre-process the raw data into a suitable form for fitting machine learning algorithms in R, all elements in the upper diagonal of each adjacency matrix was converted to a vector in major-column order. This means that the columns or predictors of the resulting data frame represents the connectivity of all unique pairwise combinations of the nodes. We repeated this process until we obtain 100 vectors (observations) and combine them (row-wise) into an R data frame. Then we added an "id" (1, 2, ..., 100) and a "response" column. The latter contains two different integer values to indicate which class each observation comes from, and the plots provide a visual illustration of the structural difference between the two classes.

```{r,cache=TRUE,message=FALSE,warning=FALSE}
## Create a custom function that generates and processes data from SBM
generate_data_SBM=function(q,n,m1,m2){
  ## Stochastic block model
  ## Generate a network with K communities and n nodes
  ## Define all variables
  K = 5
  Adj_list_1 <- list() # Adjacency lists for the 2 classes
  Adj_list_2 <- list()
  SBM_results <- list()

  ## Define the block model
  z = kronecker(1:5, rep(1, n/K))
  p1 = 0.5
  p2 = 0.1
  B = (p1 - p2) *diag(K) + p2 ## Default value of q is 0

  ## Class 1
  for(i in 1:m1){
    sbm <- sample_sbm(n, B, block.sizes = rep(n/K, K))
    Adj_list_1[[i]] <- get.adjacency(sbm)
  }
  
  SBM_results$adj_1=Adj_list_1[[1]] # Store adjacency list for Class 1

  ## Class 2
  ## First change one of the sub-blocks using the value of q for Class 2
  B[1,3] = q
  B[3, 1] = q

  for(j in 1:m2){
    sbm <- sample_sbm(n, B, block.sizes = rep(n/K, K))
    Adj_list_2[[j]] <- get.adjacency(sbm)
  }
  
  SBM_results$adj_2=Adj_list_2[[1]] # Store adjacency list for Class 2

  ## Simulated data:
  Adj_list <- c(Adj_list_1, Adj_list_2)
  class_label <- c(rep(1, m1), rep(-1, m2))

  ## Convert to a 100 x 4950 dataframe
  SBM_sims_df=c() # List of all sims
  SBM_sims_df=Reduce(rbind, lapply(Adj_list, matrix_to_vec))
  SBM_sims_df=as.data.frame(SBM_sims_df)

  ## Adding columns for response variable and id
  SBM_sims_df$id=c(1:100)
  SBM_sims_df$response=class_label
  SBM_sims_df=SBM_sims_df[,c(4951,4952,1:4950)] # Rearranging the data so that the id and response show up as the first and second columns of the data frame, respectively
  
  SBM_results$df=SBM_sims_df

  return(SBM_results)
}

## Generate the data set
set.seed(45)
SBM_sims=generate_data_SBM(q=0.4,n=100,m1=50,m2=50)
SBM_sims_df=SBM_sims$df


## Plot the adjacency list for the two classes
levelplot(as.matrix(SBM_sims$adj_1))
levelplot(as.matrix(SBM_sims$adj_2))
```

## Training and Testing Net Random Forest (NRF) Models

In this step, we will evaluate and compare the prediction performances of four different random forest models, two of which integrate spectral clustering. We named the latter models as Net Random Forest (NRF) and categorized them based on where the spectral clustering is performed: Net Random Forest Ver. 1 (NRF1) implements clustering outside of all trees while Net Random Forest Ver. 2 (NRF2) implements clustering within each tree. The other two types of random forest models are the following: (standard) Random Forest and Randomer Forest (RerF), which constructs recursive binary hyperplanes that are not necessarily axis-aligned. We will use the netRandomForest::nrf1_training, netRandomForest::nrf2_training, randomForest::randomForest, and rerf::RerF functions to train forests with B = 10 trees, importance set to "TRUE" in the randomForest() function (the variable importances for NRF2 will be manually calculated - more details in the next section; however, we will not be analyzing variable importance for RerF and NRF1), m = sqrt(p) for Random Forest, m = 0.8*n for both NRF models, and an unspecified m for Rerf (m represents the number of candidate variables at each split). The number of clusters for the spectral clustering algorithm in the NRF models will be fixed at 3. [Note: I could not find a parameter that allows the programmer to specify m in RerF() function]. 

As for the data partition process for training and testing the above models, we will implement data splitting (75% training and 25% testing), stratified on the response variable, and the splits will remain the same for all 5 replications. Each rep is a newly generated SBM data set for q values ranging from 0.2 to 0.5. As the q value increases, the two classes of the SBM data will become more distinguishable and, hence, should result in better classification accuracy across all RF models.

```{r,cache=TRUE,message=FALSE,warning=FALSE}
## Perform data splitting 
testing_data_ind = createDataPartition(SBM_sims_df$response, p = .25, list = FALSE) %>% as.vector(.)
training_data_ind = (1:nrow(SBM_sims_df))[-testing_data_ind]

## Create a matrix to store the results of the simulation to later be converted to a dataframe object
### col 1 = "Sim", col 2 = "q value" (0.2-0.5, inclusive), col 3 = "RF" (Standard RF model), col 4 = "NRF1" (RF model with clustering outside the trees), col 5 = "NRF2" (RF model with clustering within each tree), col 6 = "RerF" (Randomer Forest model)

rf_methods_all=matrix(ncol=6,nrow=5*4) # 5 reps * 4 different q values

## Define constant variables outside of all loops
num_clusters=3
index=0 # Initializing row index for the rf_methods_all table

## Obtain the accuracies for a given p1 value, then populate the sims table
for(j in c(1:5)){ # 5 replications
  
  q_vals=c(.2,.3,.4,.5)
  
  for(i in 1:length(q_vals)){
    
    index=index+1
    
    rf_methods_all[index,1]=j # Store sim number
    
    q=q_vals[i]
    rf_methods_all[index,2]=q
    
    ## Randomly generate a dataset
    set.seed(j)
    SBM_sims=generate_data_SBM(q=q,n=100,m1=50,m2=50)
    SBM_sims_df=SBM_sims$df
    
    ## Method 1: Standard random forest
  
    training_data=SBM_sims_df[training_data_ind,]
    testing_data=SBM_sims_df[testing_data_ind,]
        
    ## Train classifier on training data 
    set.seed(45)
    rf_mod1 <- randomForest(x=training_data[,c(3:ncol(training_data))],
                       y=factor(training_data[,2]),
                       ntree=10,
                       importance=TRUE)
        
    ## Test classifier on testing data
    test=predict(rf_mod1,newdata=testing_data[,c(3:ncol(testing_data))],type='response')
    
    ## Store results
    rf_methods_all[index,3]=1-mean(test != testing_data$response) # Store classification accuracy of the standard RF method
    
  
    ## Method 2: Custom random forest method with cluster outside the trees
        
    rf_mod2=nrf1_training(B=10,n=length(training_data_ind),X_training=training_data[,c(3:ncol(training_data))],Y_training=training_data$response,num_clusters=num_clusters,prop_X=.8,method="class")
    
    test=nrf1_testing(X_testing=testing_data[,c(3:ncol(testing_data))],Y_testing=testing_data$response,nrf1_mod=rf_mod2,num_clusters=num_clusters,method="class")
    
    rf_methods_all[index,4]=1-mean(test != testing_data$response)
  
    
    ## Method 3: Custom random forest method with cluster within the trees
        
    rf_mod3=nrf2_training(B=10,n=length(training_data_ind),X_training=training_data[,c(3:ncol(training_data))],Y_training=training_data$response,num_clusters=num_clusters,prop_X=.8,method="class") 
    test=nrf2_testing(X_testing=testing_data[,c(3:ncol(testing_data))],Y_testing=testing_data$response,nrf2_mod=rf_mod3,num_clusters=num_clusters,method="class")
    
    rf_methods_all[index,5]=1-mean(test != testing_data$response)
    
    
    ## Method 4: RerF
    
    rf_mod4 <- RerF(X=training_data[,c(3:ncol(training_data))], 
                    Y=factor(training_data$response), 
                    trees=10,
                    num.cores = 1L,
                    seed = 45)
    
    test <- Predict(testing_data[,c(2:ncol(testing_data))], rf_mod4, num.cores = 1L, Xtrain = 
                    training_data[,c(3:ncol(training_data))])
    
    rf_methods_all[index,6] <- 1-mean(test != testing_data$response)
  }
}

## Convert to a dataframe object, add column names, and display the results (should only have the first 5 rows populated so far since I only did one sim)
rf_methods_all=data.frame(rf_methods_all)
colnames(rf_methods_all)=c("Sim","q_value","RF","NRF1","NRF2","RerF")

## Convert the dataframe object from wide to long format to plot the results
rf_methods_allw=rf_methods_all
rf_methods_allw=rf_methods_allw %>%
                  group_by(q_value) %>%
                  summarise(RF.Mean = mean(RF),RF.SD = sd(RF), NRF1.Mean = mean(NRF1),NRF1.SD = 
                            sd(NRF1), NRF2.Mean = mean(NRF2),NRF2.SD = sd(NRF2),
                            RerF.Mean = mean(RerF),RerF.SD = sd(RerF))

rf_methods_alll=rf_methods_allw %>% 
                  gather(v, value, RF.Mean:RerF.SD) %>% 
                  separate(v, c("RF_Method", "col")) %>% 
                  arrange(q_value) %>% 
                  spread(col, value)

## Display the results
rf_methods_alll


## Graphing the results
ggplot(rf_methods_alll,aes(x=q_value,y=Mean,colour=RF_Method,group=RF_Method)) +
  geom_line(size=1) +
  labs(title = "Random Forest Models vs. q Value\n", 
       x = "q Value", 
       y = "Mean Classification Accuracy", 
       color = "RF Method\n") +
  theme(plot.title = element_text(hjust = 0.5))
```

From this plot, we can see that the both the NRF1 and NRF2 methods significantly outperforms all the other random forest methods with perfect classification at q values of 0.3 to 0.5.


## Variable Importance and Classification Performance

Now that we have assessed the predictive performance of four different random forest methods, we will now identify and visualize the important variables (i.e. predictors that have more influence on the response) from the NRF2 and standard RF models. These quantities will be computed using the following variable importance metric: mean decrease in the Gini index of node impurity by splits for each variable. In simpler terms, this is a measure of how well a variable (or node) of a decision tree classifies the data every time a split is introduced to it and is averaged across all trees that select the variable as a split candidate in the random forest. However, as previoiusly mentioned, manual calculations will be required for aggregating the gini impurity of each variable in the NRF2 model since the netRandomForest::nrf2_training function only returns the variable importances for each tree separately. This process will be repeated for various q values at K=5 clusters. 

After calculating the variable importances for both models, we also measured their classification performance in terms of the 'degree' of importance for each variable (scaled to be between 0 to 1) relative to those of the 'true' matrix (i.e. how 'well' the variables match up with those of the 'true' matrix in regards to their mean decrease in gini impurity). The latter is defined as a matrix that contains 1's within the (3,1) and (1,3) "block" regions and 0's everywhere else. This performance can be visually detected by an ROC curve, which is computed for each q value. More specifically, we would expect that larger q values would yield ROC curves that are closer to the upper-left corner and are associated with better classifiers. 

```{r}
## Obtain variable importance measures at K=5 clusters and q=0.2...0.5 for NRF2 (B=10) and the standard RF (B=1000) models. Compare their classification performances based on how 'close' the importance variables match up with those of the 'true' matrix
num_clusters=5

## Function for scaling a feature to have a range of 0 to 1
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
## Citation: https://stackoverflow.com/questions/5665599/range-standardization-0-to-1-in-r
## [Note: Is this is how I would include citations within vignettes?]

## Define the 'true' matrix; that is, the matrix with perfect assignment of important variables
correct_mat=matrix(0,100,100)
correct_mat[c(0:20),c(40:60)]=1; correct_mat[c(40:60),c(0:20)]=1
plot_adjmatrix(correct_mat)

for(q in 1:length(q_vals)){
  
  ## Randomly generate a dataset
  set.seed(q)
  SBM_sims=generate_data_SBM(q=q_vals[q],n=100,m1=50,m2=50)
  SBM_sims_df=SBM_sims$df
  
  training_data=SBM_sims_df[training_data_ind,]
  testing_data=SBM_sims_df[testing_data_ind,]
  
  NRF2_mod=nrf2_training(B=10,n=length(training_data_ind),X_training=training_data[,c(3:ncol(training_data))],Y_training=training_data$response,num_clusters=num_clusters,prop_X=.8,method="class")
  
  vec_VI_list=list() # Define a list that stores the variable importances of each tree in the NRF2 model
  VI_mat=matrix(0,100,100); V_mat=matrix(0,100,100); S_mat=matrix(0,100,100) # Initialize V, VI, and S matrices
  
  ## Create a dataframe that scores all the variable importances from each tree
  VI_df=data.frame(matrix(0,nrow=10,ncol=(num_clusters*(num_clusters-1))/2 + num_clusters))
  
  for(b in 1:length(NRF2_mod$rf)){
    ## Create a matrix Z that indicates which cluster each node/observation belongs to
    ## Obtain the clustering memberships of all training data based on bth tree
    Z_mat=matrix(0,100,num_clusters)
    included_vert=NRF2_mod$m[[b]]
    for(i in c(1:80)){
      clust_num=NRF2_mod$clusters[[b]][i]
      if(clust_num==1){
        Z_mat[included_vert[i],1]=1
      } else if (clust_num==2){
        Z_mat[included_vert[i],2]=1
      } else if (clust_num==3){
        Z_mat[included_vert[i],3]=1
      } else if (clust_num==4){
        Z_mat[included_vert[i],4]=1
      } else {
        Z_mat[included_vert[i],5]=1
      }
    }
    
    vec_VI=NRF2_mod$rf[[b]]$variable.importance
    df_importance = data.frame(matrix(0, ncol = 15))
    df_importance[, names(vec_VI)]=vec_VI
    vec_VI_list[[b]]=as.numeric(df_importance)
    
    ## Create the C matrix
    C_mat <- matrix(0, num_clusters, num_clusters)
    C_mat[upper.tri(C_mat, diag = TRUE)] = vec_VI_list[[b]]
    C_mat=C_mat+t(C_mat)
    diag(C_mat)=diag(C_mat)/2
    
    ## Do a cumulative elementwise sum of all entries of each matrix
    V_mat=V_mat+(Z_mat %*% C_mat %*% t(Z_mat))
    S_mat=S_mat+tcrossprod(rowSums(Z_mat))
  }
  
  ## Find the means VI's for all nodes in the training set
  for(i in 1:nrow(V_mat)){
    for(j in 1:ncol(V_mat)){
      if(S_mat[i,j]!=0){
        VI_mat[i,j]=V_mat[i,j]/S_mat[i,j]
      }
    }
  }
  
  ## Scale VI_mat
  VI_mat=range01(VI_mat)
  
  ## Obtain the same plot for standard RF algorithm
  set.seed(45)
  standard_rf_mod <- randomForest(x=training_data[,c(3:ncol(training_data))],
                         y=factor(training_data[,2]),
                         ntree=1000,
                         importance=TRUE)
  scaled_rf_importance=range01(standard_rf_mod$importance[,4])
  
  ## Obtain the binary matrix (predicted matrix) by subsetting the original variable importance matrix (which is plotted) by variables selected from a given threshold, tau; the 'true' matrix will contain 0's everywhere with 1's within the (3,1) and (1,3) block regions
  
  ## Plot ROC curve for NRF2
  pred_NRF2 <- prediction(matrix_to_vec(VI_mat), matrix_to_vec(correct_mat))
  perf_NRF2 <- performance(pred_NRF2,"tpr","fpr")
  plot(perf_NRF2, col="red")
  
  ## Plot ROC curve for standard RF
  pred_RF <- prediction(scaled_rf_importance, matrix_to_vec(correct_mat))
  perf_RF <- performance(pred_RF,"tpr","fpr")
  plot(perf_RF,add=TRUE,col="blue")
  
  ## Add Legend
  legend("bottomright", c("NRF2", "RF"), lty=1, 
      col = c("red", "blue"), bty="n")
  
  title_name=paste("q =",q_vals[q])
  
  ## Add title 
  title(main = title_name)
}
```
Indeed, the above hypothesis is confirmed by the plotted ROC curves with NRF2 consistently having better performance than the standard RF model. 

```{r}
## Plot the predicted matrices of NRF2 and standard RF at q=0.4, respectively
plot_adjmatrix(VI_mat)
plot_adjmatrix(scaled_rf_importance)
```

The predicted matrices of both models also indicate that the (3,1); (1,3) 'blocks' are much more distinguishable from all the other regions for NRF2 compared to those of the standard RF model.

